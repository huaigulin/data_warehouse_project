# Data Warehouse Project

- The purpose of the project is to apply what I have learnt on data warehouses and AWS, and build an ETL pipeline for a database hosted on Amazon Redshift.

## Background

- An imaginary startup called Sparkify has a new music streaming app, and the company's data analytics team is interested in knowing what songs its users are listening to.

- Sparkify has two datasets before this project:

  1. **The song dataset** is a subset of data from [Million Song Dataset](http://millionsongdataset.com/). It consists of JSON files and each contains metadata about a song and its artist.

  1. **The log dataset** is generated by this [event simulator](https://github.com/Interana/eventsim), and it simulates user activity logs from the music streaming app.

- Since it is not easy to query song play data from these two datasets, my job is to build an ETL pipeline to create a database on Amazon Redshift to help the data analytics team analyze song play data.

## Database Schema

- For the purpose of song play analysis, I create a [star schema](https://en.wikipedia.org/wiki/Star_schema) in which there are a fact table _songplays_ and four dimension tables _users_, _songs_, _artists_ and _time_:

  1. The _songplays_ table holds records in the log dataset that are associated with song plays i.e. records with page `NextSong`. The table has the following columns:

     | Column      | Data Type            | Comment                                                                                                                                                                                                 |
     | ----------- | -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
     | songplay_id | BIGINT IDENTITY(0,1) | Auto-generated when a row is inserted into the table.                                                                                                                                                   |
     | start_time  | TIMESTAMP            | The timestamp that the user starts to play the song, which corresponds to the `ts`(timestamp) data from **the log dataset**.                                                                            |
     | user_id     | INTEGER              | The user's id, which corresponds to the `userId` data from **the log dataset**.                                                                                                                         |
     | level       | VARCHAR              | The user's level(subscription type), which corresponds to the `level` data from **the log dataset**.                                                                                                    |
     | song_id     | VARCHAR              | The id of the song that's been played, which corresponds to the `song_id` data from **the song dataset** and can be retreived from _songs_ table after joining _songs_ and _artists_ tables.            |
     | artist_id   | VARCHAR              | The artist id of the song that's been played, which corresponds to the `artist_id` data from **the song dataset** and can be retreived from _artists_ table after joining _songs_ and _artists_ tables. |
     | session_id  | INTEGER              | The id of the user's playing session, which corresponds to the `sessionId` data from **the log dataset**.                                                                                               |
     | location    | VARCHAR              | The location of the user, which corresponds to the `location` data from **the log dataset**.                                                                                                            |
     | user_agent  | VARCHAR              | The device that the user used to play the song, which corresponds to the `userAgent` data from **the log dataset**.                                                                                     |

  1. The _users_ table holds data for users of the app. It has the following columns:

     | Column     | Data Type |
     | ---------- | --------- |
     | user_id    | INTEGER   |
     | first_name | VARCHAR   |
     | last_name  | VARCHAR   |
     | gender     | CHAR      |
     | level      | VARCHAR   |

  1. The _songs_ table holds data for all the songs in **the song dataset**. The columns are:

     | Column    | Data Type |
     | --------- | --------- |
     | song_id   | VARCHAR   |
     | title     | VARCHAR   |
     | artist_id | VARCHAR   |
     | year      | INTEGER   |
     | duration  | NUMERIC   |

  1. The _artists_ table holds data for all the artists in **the song dataset**. The columns are:

     | Column    | Data Type |
     | --------- | --------- |
     | artist_id | VARCHAR   |
     | name      | VARCHAR   |
     | location  | VARCHAR   |
     | latitude  | NUMERIC   |
     | longitude | NUMERIC   |

  1. The _time_ table is the timestamps of records in _songplays_ broken down into specific units. The columns are:

     | Column     | Data Type |
     | ---------- | --------- |
     | start_time | TIMESTAMP |
     | hour       | INTEGER   |
     | day        | INTEGER   |
     | week       | INTEGER   |
     | month      | INTEGER   |
     | year       | INTEGER   |
     | weekday    | INTEGER   |

- Once the data is loaded into the database, we can reasonably predict that:

  - the fact table _songplays_ will have most number of rows, since a song can be played by many users for many times
  - there should also be much more songs than artists, so the _songs_ table will be much bigger than _artists_ table
  - as a startup, Sparkify might not have a lot of users, so the _users_ table will not be very big
  - the size of _time_ table depends on how average number of music-listeners per second

- Based on these predictions, I choose the following distribution strategies for the Redshift cluster:

  - Define the `song_id` columns in _songplays_ and _songs_ table as DISTKEY, to **speed up frequent joins between these two big tables** and **balance workload**.

  - Set DISTSTYLE to ALL for _artists_ and _user_ table, so these tables will be distributed to all nodes. By using this strategy, JOIN queries between these two tables and the _songplays_ table will be much faster, but the storage cost won't increase too much since these two tables are not big.

  - Set DISTSTYLE to EVEN for _time_ table. Once the app is popular, there could be many users using the app at the same time, so it is better to distribute the table evenly across nodes to balance workload. The EVEN distribution does impact the performance of JOIN queries between _time_ and _songplay_ table. However, the main perpose of the _time_ table is to offer analytical insights into user behaviors like how many users are listening to music during each day of a week. Converting a single timestamp to a desired format can be easily done programatically, and there is no need to do JOINs.

## ETL Pipeline

- `create_tables.py` file runs DROP TABLE queries to drop the existing tables first, then creates 2 staging tables `staging_events` and `staging_songs`, and 5 star schema tables by runing CREATE TABLE queries.

- `etl.py` file runs COPY queries to load data from **the log dataset** and **the song dataset** into the 2 staging tables, then runs INSERT queries to extract and insert data into the star schema. Note that _songplay_, _user_, _song_ and _artist_ table use SELECT DISTINCT, but _time_ table only uses SELECT. This is because user, song and artist entries should obviously not have any duplicate, and one user is also unlikely to play the same song multiple times in a single second. However, _time_ table should allow duplicates since it can show how many users listening to songs at the same time.

## How to Run

1. Create an IAM Role and a Amazon Redshift cluster on AWS and get `HOST`, `DB_NAME`, `DB_USER`, `DB_PASSWORD`, `DB_PORT` and `ARN` information. Fill out the information in `dwh.cfg` config file.

1. Create tables in Redshift:

   ```
   python create_tables.py
   ```

1. Run ETL pipeline:

   ```
   python etl.py
   ```

1. Run analytics queries in the Redshift query editor.
